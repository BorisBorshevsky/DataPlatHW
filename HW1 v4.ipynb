{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcxOIdPuj4zS"
   },
   "source": [
    "# HW 1 - Data Platform IDC\n",
    "ID1: 308564293\n",
    "\n",
    "ID2: 311898746"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qr7oPwN2kp01"
   },
   "source": [
    "## Step 1: Setup Cassandra Cluster locally -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "2RtlqrV1ktnv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> Code is in markdown as we don't want to run this from the notebook\n",
    "> The setup is oriented for mac env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "KTMrqGmDkw3b",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Make sure Docker Desktop is installed. If not, download and install Docker from the official website [Get Docker | Docker Docs](https://docs.docker.com/get-docker/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Download docker image for cassandra\n",
    "```bash\n",
    "docker pull cassandra:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run container locally open for conncetion from the notebook\n",
    "```bash\n",
    "docker run --volume=/var/lib/cassandra --restart=no -p 127.0.0.1:9042:9042 -p 127.0.0.1:9160:9160 --name hw-cass -d cassandra:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Connect to run `cqlsh` locally\n",
    "```bash\n",
    "docker exec -it hw-cass cqlsh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 2: setup python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "install python\n",
    "```bash\n",
    "brew install python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "install required libraries\n",
    "\n",
    "Install libraries\n",
    "```bash\n",
    "pip3 install cassandra-driver\n",
    "pip3 install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "uvV_bF1Elk9I",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 3: Chossing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Data set content - [Dataset link.](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data)\n",
    "The Book-Crossing dataset has 3 files and is given in .csv form.\n",
    "\n",
    "\n",
    "`Users` Has 3 colums:\n",
    "* User-ID (int): Anonymized user identifier mapped to integers.\n",
    "* Location (text): Demographic information about the user's location.\n",
    "* Age (float): Age of the user. May contain NULL-values if not available.\n",
    "\n",
    "Rows:  279K, each identifing a user.\n",
    "\n",
    "`Books` Has 8 columns:\n",
    "* SBN (text): Unique identifier for each book.\n",
    "* Book-Title (text): Title of the book.\n",
    "* Book-Author (text): Author of the book. In case of multiple authors, only the first one is provided.\n",
    "* Year-Of-Publication (int): The year when the book was published.\n",
    "* Publisher (text): The publisher of the book.\n",
    "* Image-URL-S, Image-URL-M, Image-URL-L (text): URLs linking to cover images in small, medium, and large sizes.\n",
    "\n",
    "Rows: 271K, that identify books.\n",
    "\n",
    "`Ratings` Has 3 columns:\n",
    "* User-ID (int): Anonymized user identifier corresponding to Users table.\n",
    "* ISBN (text): Unique identifier corresponding to Books table.\n",
    "* Book-Rating (int): User's rating for a particular book. Explicit ratings are on a scale from 1-10, while implicit ratings are expressed as 0.\n",
    "\n",
    "Rows: 1.14M, each representing an interaction where a user rated a specific (single) book.\n",
    "\n",
    "\n",
    "#### Dataset Selection\n",
    "* The dataset was selected for it's very large size (14 columsn and over 1.5M rows) that will allow us to explore Cassandra's strengths with large datasets.\n",
    "* The dataset is also realistic, reflecting a real-life scenario of different data soruces and systems that are interconnected. \n",
    "* Data is relatively well organized and clean.\n",
    "\n",
    "**In this project:** The data set is stored in the `Data` folder - where the names of the files are corresponding to the explnation above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 4: Cassendra Database design\n",
    "Our dataset will consist of three Cassandra tables that correspond to the different entities in the DB: `Ratings`, `Books` and `Users`.\n",
    "\n",
    "Each table has a different purpose, information, keys and columns. \n",
    "Further, We can assume each entity has originated form a differnet data collection method, so it makes senese to keep them separated in three tables.\n",
    "\n",
    "##### `book_ratings`\n",
    "* **Structure**: User-ID: int, ISBN: text, Book-Rating: int\n",
    "* **Primary key**: (User-ID, ISBN)\n",
    "* **Partition Key**: User-ID\n",
    "* **Clustering Column**: ISBN\n",
    "\n",
    "Since we need a pair of user ID and ISBN to identify a transaction, they are both included in the primary key.\n",
    "    \n",
    "The partition key is selected to be User-ID as it has high cardinality (even spread of data).\n",
    "\n",
    "##### `books`\n",
    "* **Structure**: ISBN: text, Book-Title: text, Book-Author: text, Year-Of-Publication: int, Publisher: text, Image-URL-S: text, Image-URL-M: text, Image-URL-L: text\n",
    "* **Primary key**: (ISBN, Book-Author, Publisher)\n",
    "* **Partition Key**: ISBN\n",
    "* **Clustering Column**: Book-Author, Publisher\n",
    "\n",
    "ISBN is a unique identifier per book so it's a good partition key. Clustering keys are seleted to be publisher and author which are expected to have multiple entries.\n",
    "\n",
    "##### `users`\n",
    "* **Structure**: User-ID: int, Location: text, Age: float (for some cases age was not present, we decided to store this as 0 since age is part of the clustering key)\n",
    "* **Primary key**: (User-ID, Location, Age)\n",
    "* **Partition Key**: User-ID\n",
    "* **Clustering Column**: Location, Age\n",
    "\n",
    "User-ID is a unique identifier per user so it's a good partition key. Clustering keys are seleted to be Location and Age which are expected aggregations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 5: Setup keyspace and tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "First - We would like to load all the libraries need for ingestion and working with the cassandra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cassandra-driver\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import SimpleStatement, BatchStatement\n",
    "\n",
    "# Data\n",
    "import csv\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Connect to our cassandra instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect Cassandra-Driver to the Cluster running on the Docker:\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Create out key space `books`.\n",
    "We will use `SimpleStrategy` and `replication_factor` = `1` as they serve us well for the purpose of this excsrsize - as we are not looking for any HA or significant scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x12d07fa30>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS books WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Make sure keyspace created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE KEYSPACE books WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"DESCRIBE books;\").one()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Use the keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.execute(\"USE books\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table creation: \n",
    "Next, we will create the tables according to the DB schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS books (\n",
    "        ISBN text,\n",
    "        Book_Title text,\n",
    "        Book_Author text,\n",
    "        Publisher text,\n",
    "        Year_Of_Publication int,       \n",
    "        Image_URL_S text,\n",
    "        Image_URL_M text,\n",
    "        Image_URL_L text,\n",
    "        PRIMARY KEY ((Book_Author, Publisher), Year_Of_Publication, isbn)\n",
    "    )\n",
    "\"\"\").one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS books2 (\n",
    "        ISBN text,\n",
    "        Book_Title text,\n",
    "        Book_Author text,\n",
    "        Publisher text,\n",
    "        Year_Of_Publication int,       \n",
    "        Image_URL_S text,\n",
    "        Image_URL_M text,\n",
    "        Image_URL_L text,\n",
    "        PRIMARY KEY ((Book_Author), publisher, Year_Of_Publication, isbn)\n",
    "    )\n",
    "\"\"\").one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS books3 (\n",
    "        ISBN text,\n",
    "        Book_Title text,\n",
    "        Book_Author text,\n",
    "        Publisher text,\n",
    "        Year_Of_Publication int,       \n",
    "        Image_URL_S text,\n",
    "        Image_URL_M text,\n",
    "        Image_URL_L text,\n",
    "        PRIMARY KEY ((Book_Author), publisher, Year_Of_Publication, isbn)\n",
    "    )\n",
    "\"\"\").one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS book_ratings (\n",
    "        User_ID int,\n",
    "        ISBN text,\n",
    "        Book_Rating int,\n",
    "        PRIMARY KEY(User_ID, ISBN)\n",
    "    )\n",
    "\"\"\").one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        User_ID int,\n",
    "        Location text,\n",
    "        Age float,\n",
    "        PRIMARY KEY (User_ID, Location, Age)\n",
    "    )\n",
    "\"\"\").one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(keyspace_name='books', type='table', name='book_ratings'),\n",
       " Row(keyspace_name='books', type='table', name='books'),\n",
       " Row(keyspace_name='books', type='table', name='books2'),\n",
       " Row(keyspace_name='books', type='table', name='books3'),\n",
       " Row(keyspace_name='books', type='table', name='users')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"\"\"DESCRIBE tables;\"\"\")[0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 6: Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Ingestion function*\n",
    "`load_data` function reads data from a CSV file, splits it into batches, and inserts the data into a table using concurrency.\n",
    "\n",
    "Batches and concurrency are important for improving the performance and efficiency of inserting large amounts of data into a database table.\n",
    "\n",
    "When inserting data into a database table, it is possible to insert row by row, or to insert multiple rows at once as a batch. Inserting data in a batch greatly improves the performance of inserting data into the database, as it reduces the number of round trips to the database and can ensure that the data is consistent. This is more efficient than inserting one row at a time, which can be slow and can lead to unnecessary overhead.\n",
    "\n",
    "Concurrency is important because it allows multiple threads to be used when inserting data into a database table. This improves performance by allowing multiple inserts to happen simultaneously, which can greatly increase the speed of inserting large amounts of data. Without concurrency, each insert operation would have to wait for the previous insert to complete, leading to a slower overall process.\n",
    "\n",
    "In summary, using batches and concurrency can greatly improve the performance and efficiency of inserting large amounts of data into a database table, resulting in faster insert times and better use of system resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to construct the batch operation and then run concurrently\n",
    "def load_data_concurrent(csv_file, insert_query, data_types, batch_size=100, concurrency=20, max_batches=None):        \n",
    "    '''Load data from a CSV file into a Cassandra table using batch inserts.\n",
    "        Inputs:\n",
    "        1. csv_file: str\n",
    "            Path to the CSV file.\n",
    "        2. insert_query: str\n",
    "            CQL query for inserting data into the Cassandra table. \n",
    "        3. data_types: list\n",
    "            List of data types corresponding to the columns in the CSV file.\n",
    "        4. batch_size: int, optional (default=100)\n",
    "            Number of rows in each batch.\n",
    "        5. concurrency: int, optional (default=20)\n",
    "            Number of threads for parallel execution.\n",
    "\n",
    "        Returns: Tuple with converted data.\n",
    "    '''\n",
    "    # Helper functions\n",
    "    def convert_data(row):\n",
    "        # Convert a single row's data to the correct format according to the data_types\n",
    "        \n",
    "        converted_data = [data_type(value) if value else data_type() for value, data_type in zip(row, data_types)]\n",
    "        return tuple(converted_data)\n",
    "\n",
    "    def import_batch(rows):\n",
    "        # Build a batch statement for the current set of rows (single batch)\n",
    "        batch = BatchStatement()\n",
    "        for row in rows:\n",
    "            # Convert data types\n",
    "            try:\n",
    "                converted_row = convert_data(row)\n",
    "                # Create the query and add it to the batch\n",
    "                \n",
    "                batch.add(insert_query, converted_row)\n",
    "            except Exception as e:\n",
    "                print(f\"error on row: {row}, skipping\")\n",
    "        session.execute(batch, trace=True)\n",
    "\n",
    "    # Function body\n",
    "    # open CSV\n",
    "    with open(csv_file, \"r\") as f:\n",
    "        next(f)  # Skip the header row.\n",
    "        reader = list(csv.reader(f))\n",
    "\n",
    "    # split to batches\n",
    "    rows = [reader[i: i + batch_size] for i in range(0, len(reader), batch_size)][:max_batches]\n",
    "\n",
    "    if concurrency > 1:\n",
    "        # concurrently run each batch\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:\n",
    "            executor.map(import_batch, rows)\n",
    "    else:\n",
    "        for batch in rows:\n",
    "            import_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Files = namedtuple(\"Files\", \"users ratings books\")\n",
    "\n",
    "files = Files(\n",
    "    users='Data/Users.csv',\n",
    "    ratings='Data/Ratings.csv',\n",
    "    books='Data/Books.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv(files.users)\n",
    "df_books = pd.read_csv(files.books)\n",
    "df_ratings = pd.read_csv(files.ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278858, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271360, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1149780, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1149780, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users_ratings = df_users.merge(df_ratings, on='User-ID', how='inner', suffixes=('_1', '_2'))\n",
    "df_users_ratings.set_index([\"User-ID\", \"ISBN\"])\n",
    "df_users_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271360, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.rename(columns={ df_books.columns[0]: \"ISBN\" }, inplace = True)\n",
    "df_books.set_index(\"ISBN\")\n",
    "df_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1031136, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users_ratings_book = df_users_ratings.merge(df_books, on='ISBN', how='inner', suffixes=('_1', '_2'))\n",
    "df_users_ratings_book.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this means we have lots of ratings for books that are not exist, lets remove them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Investigate the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(file_name):\n",
    "    with open(file_name, \"r\") as file:\n",
    "        return len(file.readlines()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file sizes are - users: 278858, ratings: 1149780, books: 271360\n"
     ]
    }
   ],
   "source": [
    "users_file_size = count_lines(files.users)\n",
    "ratings_file_size = count_lines(files.ratings)\n",
    "books_file_size = count_lines(files.books)\n",
    "\n",
    "print(f\"file sizes are - users: {users_file_size}, ratings: {ratings_file_size}, books: {books_file_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_records(table):\n",
    "    count_query = f\"SELECT COUNT(1) FROM {table}\"\n",
    "    result = session.execute(count_query)\n",
    "    return result.one().count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ingestion:\n",
    "Now we will ingest the data using the `load_data_concurrent` fucntion and validate for each table that the number of rows matches the number in the CSV.\n",
    "\n",
    "##### `book_ratings`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m ratings_data_types \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]  \u001b[38;5;66;03m# User_ID, ISBN, Book_Rating\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mload_data_concurrent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratings_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratings_data_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 54\u001b[0m, in \u001b[0;36mload_data_concurrent\u001b[0;34m(csv_file, insert_query, data_types, batch_size, concurrency, max_batches)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[0;32m---> 54\u001b[0m         \u001b[43mimport_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m, in \u001b[0;36mload_data_concurrent.<locals>.import_batch\u001b[0;34m(rows)\u001b[0m\n\u001b[1;32m     31\u001b[0m     converted_row \u001b[38;5;241m=\u001b[39m convert_data(row)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Create the query and add it to the batch\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror on row: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, skipping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/IdeaProjects/DataPlatHW/demo/lib/python3.9/site-packages/cassandra/query.py:820\u001b[0m, in \u001b[0;36mcassandra.query.BatchStatement.add\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/IdeaProjects/DataPlatHW/demo/lib/python3.9/site-packages/cassandra/encoder.py:60\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mfloat\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_float,\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mbytearray\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_bytes,\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mstr\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_str,\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mint\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_object,\n\u001b[1;32m     65\u001b[0m         UUID: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_object,\n\u001b[1;32m     66\u001b[0m         datetime\u001b[38;5;241m.\u001b[39mdatetime: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_datetime,\n\u001b[1;32m     67\u001b[0m         datetime\u001b[38;5;241m.\u001b[39mdate: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_date,\n\u001b[1;32m     68\u001b[0m         datetime\u001b[38;5;241m.\u001b[39mtime: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_time,\n\u001b[1;32m     69\u001b[0m         Date: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_date_ext,\n\u001b[1;32m     70\u001b[0m         Time: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_time,\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mdict\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_map_collection,\n\u001b[1;32m     72\u001b[0m         OrderedDict: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_map_collection,\n\u001b[1;32m     73\u001b[0m         OrderedMap: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_map_collection,\n\u001b[1;32m     74\u001b[0m         OrderedMapSerializedKey: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_map_collection,\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28mlist\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_list_collection,\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28mtuple\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_list_collection,  \u001b[38;5;66;03m# TODO: change to tuple in next major\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28mset\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_set_collection,\n\u001b[1;32m     78\u001b[0m         sortedset: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_set_collection,\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28mfrozenset\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_set_collection,\n\u001b[1;32m     80\u001b[0m         types\u001b[38;5;241m.\u001b[39mGeneratorType: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_list_collection,\n\u001b[1;32m     81\u001b[0m         ValueSequence: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_sequence,\n\u001b[1;32m     82\u001b[0m         Point: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_str_quoted,\n\u001b[1;32m     83\u001b[0m         LineString: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_str_quoted,\n\u001b[1;32m     84\u001b[0m         Polygon: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_str_quoted\n\u001b[1;32m     85\u001b[0m     }\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28mmemoryview\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_bytes,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mbytes\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_bytes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m         ipaddress\u001b[38;5;241m.\u001b[39mIPv6Address: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcql_encode_ipaddress\n\u001b[1;32m     93\u001b[0m     })\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## book_ratings table\n",
    "# Insertion Query\n",
    "ratings_query = \"\"\"\n",
    "            INSERT INTO book_ratings (User_ID, ISBN, Book_Rating)\n",
    "            VALUES (%s, %s, %s)\n",
    "            \"\"\"\n",
    "\n",
    "# A list of data types for this table\n",
    "ratings_data_types = [int, str, int]  # User_ID, ISBN, Book_Rating\n",
    "\n",
    "# load data\n",
    "load_data_concurrent(files.ratings, ratings_query, ratings_data_types, concurrency=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Book ratings - Validate all inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqgzndRPl8ig"
   },
   "outputs": [],
   "source": [
    "# Row count validation\n",
    "cass_len = num_of_records(\"book_ratings\")\n",
    "line_count = count_lines(files.ratings)\n",
    "\n",
    "assert cass_len == line_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `users`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertion Query\n",
    "users_query = \"\"\"\n",
    "                INSERT INTO users (User_ID, Location, Age)\n",
    "                VALUES (%s, %s, %s)\n",
    "                \"\"\"\n",
    "\n",
    "# A list of data types for this table\n",
    "users_data_types = [int, str, float]  # User_ID, Location, Age\n",
    "\n",
    "# load data\n",
    "load_data_concurrent(files.users, users_query, users_data_types, concurrency=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row count validation\n",
    "row_count = num_of_records(\"users\")\n",
    "line_count = count_lines(files.users)\n",
    "\n",
    "assert row_count == line_count , f\"{row_count} != {line_count}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `books`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## books table\n",
    "# Insertion Query\n",
    "books_query = \"\"\"\n",
    "            INSERT INTO books2 (ISBN, Book_Title, Book_Author, Year_Of_Publication, Publisher, Image_URL_S, Image_URL_M, Image_URL_L)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "\n",
    "# A list of data types for this table\n",
    "books_data_types = [str, str, str, int, str, str, str, str]  # ISBN,Book-Title,Book-Author,Year-Of-Publication,Publisher,Image-URL-S,Image-URL-M,Image-URL-L\n",
    "\n",
    "# load data\n",
    "load_data_concurrent(files.books, books_query, books_data_types, batch_size=100, concurrency=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation - Books table - all data is inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_of_records' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Row count validation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m row_count \u001b[38;5;241m=\u001b[39m \u001b[43mnum_of_records\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbooks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m line_count \u001b[38;5;241m=\u001b[39m count_lines(files\u001b[38;5;241m.\u001b[39mbooks)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m row_count \u001b[38;5;241m==\u001b[39m line_count , \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_of_records' is not defined"
     ]
    }
   ],
   "source": [
    "# Row count validation\n",
    "row_count = num_of_records(\"books\")\n",
    "line_count = count_lines(files.books)\n",
    "\n",
    "assert row_count == line_count , f\"{row_count} != {line_count}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RITM59YXmH4j"
   },
   "source": [
    "## Step 7: Data investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Book distribution by publisher - J\n",
    "- Book distribuiton by year - J\n",
    "- Book distribuiton by author - J\n",
    "- Avarage rating per user - J\n",
    "- Avarage rating per publisher - J\n",
    "- Popular Authors by Number of Ratings\n",
    "- Books with Highest Ratings and Most Ratings - ?\n",
    "\n",
    "- top rated book each year - B\n",
    "- top rated books by age - B \n",
    "- top rated authors by age - B,"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
