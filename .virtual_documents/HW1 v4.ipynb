












































# Cassandra-driver
from cassandra.cluster import Cluster
from cassandra.query import SimpleStatement, BatchStatement

# Data
import csv
import pandas as pd
import concurrent.futures
from collections import namedtuple





# Connect Cassandra-Driver to the Cluster running on the Docker:
cluster = Cluster(['127.0.0.1'])
session = cluster.connect()





session.execute("CREATE KEYSPACE IF NOT EXISTS books WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };")





session.execute("DESCRIBE books;").one()[3]





session.execute("USE books");





session.execute("""
    CREATE TABLE IF NOT EXISTS books (
        ISBN text,
        Book_Title text,
        Book_Author text,
        Publisher text,
        Year_Of_Publication int,       
        Image_URL_S text,
        Image_URL_M text,
        Image_URL_L text,
        PRIMARY KEY ((Book_Author, Publisher), Year_Of_Publication, isbn)
    )
""").one()


session.execute("""
    CREATE TABLE IF NOT EXISTS books2 (
        ISBN text,
        Book_Title text,
        Book_Author text,
        Publisher text,
        Year_Of_Publication int,       
        Image_URL_S text,
        Image_URL_M text,
        Image_URL_L text,
        PRIMARY KEY ((Book_Author), publisher, Year_Of_Publication, isbn)
    )
""").one()


session.execute("""
    CREATE TABLE IF NOT EXISTS books3 (
        ISBN text,
        Book_Title text,
        Book_Author text,
        Publisher text,
        Year_Of_Publication int,       
        Image_URL_S text,
        Image_URL_M text,
        Image_URL_L text,
        PRIMARY KEY ((Book_Author), publisher, Year_Of_Publication, isbn)
    )
""").one()


session.execute("""
    CREATE TABLE IF NOT EXISTS book_ratings (
        User_ID int,
        ISBN text,
        Book_Rating int,
        PRIMARY KEY(User_ID, ISBN)
    )
""").one()


session.execute("""
    CREATE TABLE IF NOT EXISTS users (
        User_ID int,
        Location text,
        Age float,
        PRIMARY KEY (User_ID, Location, Age)
    )
""").one()





session.execute("""DESCRIBE tables;""")[0:]








# Helper function to construct the batch operation and then run concurrently
def load_data_concurrent(csv_file, insert_query, data_types, batch_size=100, concurrency=20, max_batches=None):        
    '''Load data from a CSV file into a Cassandra table using batch inserts.
        Inputs:
        1. csv_file: str
            Path to the CSV file.
        2. insert_query: str
            CQL query for inserting data into the Cassandra table. 
        3. data_types: list
            List of data types corresponding to the columns in the CSV file.
        4. batch_size: int, optional (default=100)
            Number of rows in each batch.
        5. concurrency: int, optional (default=20)
            Number of threads for parallel execution.

        Returns: Tuple with converted data.
    '''
    # Helper functions
    def convert_data(row):
        # Convert a single row's data to the correct format according to the data_types
        
        converted_data = [data_type(value) if value else data_type() for value, data_type in zip(row, data_types)]
        return tuple(converted_data)

    def import_batch(rows):
        # Build a batch statement for the current set of rows (single batch)
        batch = BatchStatement()
        for row in rows:
            # Convert data types
            try:
                converted_row = convert_data(row)
                # Create the query and add it to the batch
                
                batch.add(insert_query, converted_row)
            except Exception as e:
                print(f"error on row: {row}, skipping")
        session.execute(batch, trace=True)

    # Function body
    # open CSV
    with open(csv_file, "r") as f:
        next(f)  # Skip the header row.
        reader = list(csv.reader(f))

    # split to batches
    rows = [reader[i: i + batch_size] for i in range(0, len(reader), batch_size)][:max_batches]

    if concurrency > 1:
        # concurrently run each batch
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
            executor.map(import_batch, rows)
    else:
        for batch in rows:
            import_batch(batch)





Files = namedtuple("Files", "users ratings books")

files = Files(
    users='Data/Users.csv',
    ratings='Data/Ratings.csv',
    books='Data/Books.csv'
)


df_users = pd.read_csv(files.users)
df_books = pd.read_csv(files.books)
df_ratings = pd.read_csv(files.ratings)



df_users.shape


df_books.shape


df_ratings.shape


df_users_ratings = df_users.merge(df_ratings, on='User-ID', how='inner', suffixes=('_1', '_2'))
df_users_ratings.set_index(["User-ID", "ISBN"])
df_users_ratings.shape


df_books.rename(columns={ df_books.columns[0]: "ISBN" }, inplace = True)
df_books.set_index("ISBN")
df_books.shape


df_users_ratings_book = df_users_ratings.merge(df_books, on='ISBN', how='inner', suffixes=('_1', '_2'))
df_users_ratings_book.shape


# this means we have lots of ratings for books that are not exist, lets remove them









def count_lines(file_name):
    with open(file_name, "r") as file:
        return len(file.readlines()) - 1


users_file_size = count_lines(files.users)
ratings_file_size = count_lines(files.ratings)
books_file_size = count_lines(files.books)

print(f"file sizes are - users: {users_file_size}, ratings: {ratings_file_size}, books: {books_file_size}")


def num_of_records(table):
    count_query = f"SELECT COUNT(1) FROM {table}"
    result = session.execute(count_query)
    return result.one().count





## book_ratings table
# Insertion Query
ratings_query = """
            INSERT INTO book_ratings (User_ID, ISBN, Book_Rating)
            VALUES (%s, %s, %s)
            """

# A list of data types for this table
ratings_data_types = [int, str, int]  # User_ID, ISBN, Book_Rating

# load data
load_data_concurrent(files.ratings, ratings_query, ratings_data_types, concurrency=1)





# Row count validation
cass_len = num_of_records("book_ratings")
line_count = count_lines(files.ratings)

assert cass_len == line_count





# Insertion Query
users_query = """
                INSERT INTO users (User_ID, Location, Age)
                VALUES (%s, %s, %s)
                """

# A list of data types for this table
users_data_types = [int, str, float]  # User_ID, Location, Age

# load data
load_data_concurrent(files.users, users_query, users_data_types, concurrency=1)





# Row count validation
row_count = num_of_records("users")
line_count = count_lines(files.users)

assert row_count == line_count , f"{row_count} != {line_count}"





## books table
# Insertion Query
books_query = """
            INSERT INTO books2 (ISBN, Book_Title, Book_Author, Year_Of_Publication, Publisher, Image_URL_S, Image_URL_M, Image_URL_L)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """

# A list of data types for this table
books_data_types = [str, str, str, int, str, str, str, str]  # ISBN,Book-Title,Book-Author,Year-Of-Publication,Publisher,Image-URL-S,Image-URL-M,Image-URL-L

# load data
load_data_concurrent(files.books, books_query, books_data_types, batch_size=100, concurrency=10)





# Row count validation
row_count = num_of_records("books")
line_count = count_lines(files.books)

assert row_count == line_count , f"{row_count} != {line_count}"






